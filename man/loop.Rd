% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/loop.R
\name{loop}
\alias{loop}
\title{Loop to computate all entropy metrics outputs}
\usage{
loop(input, m, n)
}
\arguments{
\item{input}{is the source data frame}

\item{m}{is the first studied X variable column number}

\item{n}{is the studied variables number}
}
\description{
Table of mutual information theory coefficients for variables association study. Computated outputs gather: X variable name, Y variable name, X information entropy, Y information entropy, Computed marginal EPMF of X, Computed marginal EPMF of Y, Chi2, Chi2 p-value, Information entropy of X, Information entropy of Y, Joint information entropy of X and Y, Conditional information entropy of Y given X, Conditional information entropy of X given Y, Mutual information of X and Y, Normalized mutual information of X and Y.
}
\references{
Shannon, C. A mathematical theory of communication. Bell Labs Tech. J. 27, 379â€“423, DOI: 10.1002/j.1538-7305.1948.tb01338.x (1948).
}
